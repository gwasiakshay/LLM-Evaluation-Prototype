# ğŸ§  LLM Evaluation Prototype

This project explores how to evaluate the performance of GenAI models on financial summarization tasks using metrics like ROUGE. It includes a hands-on pipeline built with Hugging Face Transformers, Python, and Streamlit â€” designed to help transition from compliance roles to AI engineering.

---

## ğŸ“Œ Objectives

- Build an end-to-end evaluation pipeline for financial text summarization
- Compare GenAI-generated summaries against reference texts using ROUGE
- Visualize evaluation results with Streamlit dashboards
- Document learnings and progress toward AI engineering readiness

---

## ğŸ› ï¸ Tech Stack

| Tool/Library       | Purpose                              |
|--------------------|--------------------------------------|
| Python             | Core scripting and data handling     |
| Hugging Face       | GenAI model loading and inference    |
| `evaluate`         | ROUGE metric computation             |
| Streamlit          | Dashboard visualization              |
| Git + GitHub       | Version control and project tracking |
| VS Code + Jupyter  | Development environment              |

---

## ğŸ“‚ Folder Structure
LLM-Evaluation-Prototype/ â”œâ”€â”€ Mini_Project/ â”‚   â”œâ”€â”€ scripts/              # Evaluation scripts â”‚   â”œâ”€â”€ data/                 # Sample inputs and outputs â”œâ”€â”€ Learning_Notes/          # Daily reflections and learnings â”œâ”€â”€ README.md                # Project overvie
